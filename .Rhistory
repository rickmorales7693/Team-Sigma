library(ggplot2)
data$booking_status <- as.factor(data$booking_status)
# Train Random Forest model
model <- randomForest(booking_status ~ ., data = data, importance = TRUE)
# Plot feature importance
importance_df <- as.data.frame(importance(model))
importance_df$Feature <- rownames(importance_df)
ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Feature Importance", x = "Features", y = "Importance (Mean Decrease in Gini)")
library(pROC)
# Predict using the trained model
pred <- predict(model, type = "prob")[,2]
# ROC curve
roc_curve <- roc(data$booking_status, pred)
# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")
# Load necessary libraries
library(keras3)
library(tensorflow)
# Assuming 'data' is your dataset after loading it
# Step 1: Convert booking_status to binary
data$booking_status <- ifelse(data$booking_status == "cancelled", 1, 0)
# Step 2: Extract month and season from arrival_date
data$arrival_date <- as.Date(data$arrival_date)
data$arrival_month <- format(data$arrival_date, "%m")
data$arrival_season <- ifelse(data$arrival_month %in% c('12', '01', '02'), "Winter",
ifelse(data$arrival_month %in% c('03', '04', '05'), "Spring",
ifelse(data$arrival_month %in% c('06', '07', '08'), "Summer", "Fall")))
# Step 3: Drop 'Booking_ID' column
data$Booking_ID <- NULL
# One-hot encode categorical variables
data <- model.matrix(~. - 1, data = data)
# Step 4: Split data into training and testing sets
set.seed(42)
train_idx <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Separate features and target
X_train <- train_data[, -ncol(train_data)]
y_train <- train_data[, ncol(train_data)]
X_test <- test_data[, -ncol(test_data)]
y_test <- test_data[, ncol(test_data)]
# Step 5: Standardize the features
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)
# Step 6: Build the neural network using Keras
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = ncol(X_train_scaled)) %>%
layer_dense(units = 32, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
reticulate::py_last_error()
# Load necessary libraries
library(keras3)
library(tensorflow)
# Assuming 'data' is your dataset after loading it
# Step 1: Convert booking_status to binary
data$booking_status <- ifelse(data$booking_status == "cancelled", 1, 0)
set.seed(123)
library(ggplot2)
true_relationship <- function(x) {return(6*x^3 + 6*x^2 - 12*x)}
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
ggplot() + geom_line(aes(x = x, y = f), color = "black")
observations <- f + rnorm(length(f), mean = 0, sd = 15)
set.seed(123)
library(ggplot2)
true_relationship <- function(x) {return(6*x^3 + 6*x^2 - 12*x)}
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
ggplot() + geom_line(aes(x = x, y = f), color = "black")
observations <- f + rnorm(length(f), mean = 0, sd = 15)
set.seed(123)
library(ggplot2)
true_relationship <- function(x) {return(6*x^3 + 6*x^2 - 12*x)}
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
ggplot() + geom_line(aes(x = x, y = f), color = "black")
observations <- f + rnorm(length(f), mean = 0, sd = 15)
model1 <- lm(observations ~ poly(x, 1))
predictions1 <- predict(model1, newdata = data.frame(x = x))
model25 <- lm(observations ~ poly(x, 25))
predictions25 <- predict(model25, newdata = data.frame(x = x))
data <- data.frame(x = x,
f = f,
observations = observations,
lm = predictions1,
pm = predictions25)
ggplot(data, aes(x = x)) +
geom_line(aes(y = f), color = "black") +
geom_point(aes(y = observations), color = "blue", shape = 1) +
geom_line(aes(y = lm), color = "red", linetype = "solid") +
geom_line(aes(y = pm), color = "orange", linetype = "solid") +
geom_point(aes(x = 1, y = data[x == 1, "lm"]), color = "red", shape=2) +
geom_point(aes(x = 1, y = data[x == 1, "pm"]), color = "orange", shape=2)
observations_new <- f + rnorm(length(f), mean = 0, sd = 15)
model1_new <- lm(observations_new ~ poly(x, 1))
predictions1 <- predict(model1_new, newdata = data.frame(x = x))
model25 <- lm(observations_new ~ poly(x, 25))
predictions25 <- predict(model25, newdata = data.frame(x = x))
data <- data.frame(x = x,
f = f,
observations = observations_new,
lm = predictions1,
pm = predictions25)
ggplot(data, aes(x = x)) +
geom_line(aes(y = f), color = "black") +
geom_point(aes(y = observations), color = "blue", shape = 1) +
geom_line(aes(y = lm), color = "red", linetype = "solid") +
geom_line(aes(y = pm), color = "orange", linetype = "solid") +
geom_point(aes(x = 1, y = data[x == 1, "lm"]), color = "red", shape = 2) +
geom_point(aes(x = 1, y = data[x == 1, "pm"]), color = "orange", shape = 2)
results1 <- data.frame(x = 1, f_pred = 0)
for (i in 1:500) {
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
temp_observations <- f + rnorm(length(x), mean = 0, sd = 15)
model1 <- lm(temp_observations ~ poly(x, 1))
results1[i, 1] <- 1
results1[i, 2] <- predict(model1, newdata = data.frame(x = 1))
}
ggplot() +
geom_line(data = data, aes(x = x, y = f), color = "black") +
geom_point(data = results1, aes(x = x, y = f_pred), color = "red", shape = 2)
results20 <- data.frame(x = 1, f_pred = 0)
for (i in 1:500){
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
temp_observations <- f + rnorm(length(x), mean = 0, sd = 15)
model20 <- lm(temp_observations ~ poly(x, 20))
results20[i, 1] <- 1
results20[i, 2] <- predict(model20, newdata = data.frame(x = 1))
}
ggplot() +
geom_line(data = data, aes(x = x, y = f), color = "black") +
geom_point(data = results20, aes(x = x, y = f_pred), color = "orange", shape = 2)
models <- vector("list", 25)
for (degree in 1:25){
model <- lm(observations ~ poly(x, degree))
models[[degree]] <- model
}
results <- data.frame(degree = 1:25, rmse = 0)
for (degree in 1:25){
predictions <- predict(models[[degree]], newdata = data.frame(x = x))
results[results$degree == degree, "rmse"] <-
sqrt((1/length(predictions)) * sum((predictions - obersvations)^2))
}
set.seed(123)
library(ggplot2)
true_relationship <- function(x) {return(6*x^3 + 6*x^2 - 12*x)}
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
ggplot() + geom_line(aes(x = x, y = f), color = "black")
observations <- f + rnorm(length(f), mean = 0, sd = 15)
model1 <- lm(observations ~ poly(x, 1))
predictions1 <- predict(model1, newdata = data.frame(x = x))
model25 <- lm(observations ~ poly(x, 25))
predictions25 <- predict(model25, newdata = data.frame(x = x))
data <- data.frame(x = x,
f = f,
observations = observations,
lm = predictions1,
pm = predictions25)
ggplot(data, aes(x = x)) +
geom_line(aes(y = f), color = "black") +
geom_point(aes(y = observations), color = "blue", shape = 1) +
geom_line(aes(y = lm), color = "red", linetype = "solid") +
geom_line(aes(y = pm), color = "orange", linetype = "solid") +
geom_point(aes(x = 1, y = data[x == 1, "lm"]), color = "red", shape=2) +
geom_point(aes(x = 1, y = data[x == 1, "pm"]), color = "orange", shape=2)
observations_new <- f + rnorm(length(f), mean = 0, sd = 15)
model1_new <- lm(observations_new ~ poly(x, 1))
predictions1 <- predict(model1_new, newdata = data.frame(x = x))
model25 <- lm(observations_new ~ poly(x, 25))
predictions25 <- predict(model25, newdata = data.frame(x = x))
data <- data.frame(x = x,
f = f,
observations = observations_new,
lm = predictions1,
pm = predictions25)
ggplot(data, aes(x = x)) +
geom_line(aes(y = f), color = "black") +
geom_point(aes(y = observations), color = "blue", shape = 1) +
geom_line(aes(y = lm), color = "red", linetype = "solid") +
geom_line(aes(y = pm), color = "orange", linetype = "solid") +
geom_point(aes(x = 1, y = data[x == 1, "lm"]), color = "red", shape = 2) +
geom_point(aes(x = 1, y = data[x == 1, "pm"]), color = "orange", shape = 2)
results1 <- data.frame(x = 1, f_pred = 0)
for (i in 1:500) {
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
temp_observations <- f + rnorm(length(x), mean = 0, sd = 15)
model1 <- lm(temp_observations ~ poly(x, 1))
results1[i, 1] <- 1
results1[i, 2] <- predict(model1, newdata = data.frame(x = 1))
}
ggplot() +
geom_line(data = data, aes(x = x, y = f), color = "black") +
geom_point(data = results1, aes(x = x, y = f_pred), color = "red", shape = 2)
results20 <- data.frame(x = 1, f_pred = 0)
for (i in 1:500){
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
temp_observations <- f + rnorm(length(x), mean = 0, sd = 15)
model20 <- lm(temp_observations ~ poly(x, 20))
results20[i, 1] <- 1
results20[i, 2] <- predict(model20, newdata = data.frame(x = 1))
}
ggplot() +
geom_line(data = data, aes(x = x, y = f), color = "black") +
geom_point(data = results20, aes(x = x, y = f_pred), color = "orange", shape = 2)
models <- vector("list", 25)
for (degree in 1:25){
model <- lm(observations ~ poly(x, degree))
models[[degree]] <- model
}
results <- data.frame(degree = 1:25, rmse = 0)
for (degree in 1:25){
predictions <- predict(models[[degree]], newdata = data.frame(x = x))
results[results$degree == degree, "rmse"] <-
sqrt((1/length(predictions)) * sum((predictions - observations)^2))
}
ggplot() +
geom_line(data = results, aes(x = degree, y = rmse), color = "black")
results <- data.frame(degree = 1:25, rmse = 0)
for (degree in 1:25){
predictions <- predict(models[[degree]], newdata = data.frame(x = x))
results[results$degree == degree, "rmse"] <-
sqrt((1/length(predictions)) * sum((predictions - f)^2))
}
ggplot() +
geom_line(data = results, aes(x = degree, y = rmse), color = "black")
model <- lm(observations ~ poly(x, ?))
# Load necessary libraries
library(dplyer)
install.packages(c("dplyr", "cluster", "factoextra"))
# Load necessary libraries
library(dplyer)
# Load necessary libraries
library(dplyr)
library(cluster)
library(factoextra)
# Load the dataset (replace with your file path)
data <- read.csv("C:\\Users\\rickm\\Merrimack\\DSE5004 - Visual Data Exploration\\Assignments\\Segmentation & Profiling\\Data\\Customer_Dataset_Data.csv")
# Select relevant colmuns for clustering
selected_columns <- c("Age", "Gender", "Education.years", "Streaming.hours",
"Owns.smart.device", "Video.streaming.add.on",
"Smart.home.devices", "Owns.gaming.console", "Annual.Household.income")
segmentation_data <- data %>%
select(all_of(selected_columns))
# Convert categorical variables to numeric (e.g., Gender, Owns.smart.device)
segmentation_data$Gender <- ifelse(segmentation_data$Gender == "Male", 1, 0)
segmentation_data$Owns.smart.device <- ifelse(segmentation_data$Owns.smart.device == "Yes", 1, 0)
segmentation_data$Video.streaming.add.on <- ifelse(segmentation_data$Video.streaming.add.on == "Yes", 1, 0)
segmentation_data$Smart.home.devices <- ifelse(segmentation_data$Smart.home.devices == "Yes", 1, 0)
segmentation_data$Owns.gaming.console <- ifelse(segmentation_data$Owns.gaming.console == "Yes", 1, 0)
# Scale numeric variables
segmentation_data_scaled <- scale(segmentation_data[, -which(names(segmentation_data) == "Annual.Household.income")])
# Apply K-means clustering
set.seed(123)
kmeans_result <- kmeans(segmentation_data_scaled, centers = 4)
# Add cluster labels to the dataset
segmentation_data$KMeans_Cluster <- kmeans_result$cluster
# Visualize K-means clustering results
fviz_cluster(kmeans_result, data = segmentation_data_scaled, geom = "point", ellipse = TRUE) +
labs(title = "K-means Clustering Results")
# Compute distance matrix
dist_matrix <- dist(segmentation_data_scaled, method = "euclidean")
# Apply hierarchical clustering
hierarchical_result <- hclust(dist_matrix, method = "ward.D2")
# Cut the tree into 4 clusters
segmentation_data$Hierarchical_Cluster <- cutree(hierarchical_result, k = 4)
# Visualize dendrogram
plot(hierarchical_result, main = "Dendrogram for Hierarchical Clustering", xlab = "", sub = "", cex = 0.6)
# Highlight clusters in dendrogram
rect.hclust(hierarchical_result, k = 4, border = 2:5)
# Summarize cluster profiles
kmeans_profile <- segmentation_data %>%
group_by(KMeans_Cluster) %>%
summarise(
Avg_Age = mean(Age, na.rm = TRUE),
Avg_Streaming_Hours = mean(Streaming.hours, na.rm = TRUE),
Avg_Income = mean(as.numeric(gsub("[$,]", "", Annual.Household.income)), na.rm = TRUE),
Smart_Device_Adoption = mean(Owns.smart.device, na.rm = TRUE),
Streaming_Addon_Adoption = mean(Video.streaming.add.on, na.rm = TRUE)
)
print(kmeans_profile)
View(data)
# Apply K-means clustering
set.seed(123)
kmeans_result <- kmeans(segmentation_data_scaled, centers = 10)
# Add cluster labels to the dataset
segmentation_data$KMeans_Cluster <- kmeans_result$cluster
# Visualize K-means clustering results
fviz_cluster(kmeans_result, data = segmentation_data_scaled, geom = "point", ellipse = TRUE) +
labs(title = "K-means Clustering Results")
# Apply K-means clustering
set.seed(123)
kmeans_result <- kmeans(segmentation_data_scaled, centers = 10)
# Add cluster labels to the dataset
segmentation_data$KMeans_Cluster <- kmeans_result$cluster
# Visualize K-means clustering results
fviz_cluster(kmeans_result, data = segmentation_data_scaled, geom = "point", ellipse = TRUE) +
labs(title = "K-means Clustering Results")
write.csv(data, "Customer_Data_With_Clusters.csv", row.names = FALSE)
# Load necessary libraries
library(dplyr)
library(cluster)
library(factoextra)
# Load the dataset (replace with your file path)
data <- read.csv("C:\\Users\\rickm\\Merrimack\\DSE5004 - Visual Data Exploration\\Assignments\\Segmentation & Profiling\\Data\\Customer_Dataset_Data.csv")
# Select relevant colmuns for clustering
selected_columns <- c("Age", "Gender", "Education.years", "Streaming.hours",
"Owns.smart.device", "Video.streaming.add.on",
"Smart.home.devices", "Owns.gaming.console", "Annual.Household.income")
segmentation_data <- data %>%
select(all_of(selected_columns))
# Convert categorical variables to numeric (e.g., Gender, Owns.smart.device)
segmentation_data$Gender <- ifelse(segmentation_data$Gender == "Male", 1, 0)
segmentation_data$Owns.smart.device <- ifelse(segmentation_data$Owns.smart.device == "Yes", 1, 0)
segmentation_data$Video.streaming.add.on <- ifelse(segmentation_data$Video.streaming.add.on == "Yes", 1, 0)
segmentation_data$Smart.home.devices <- ifelse(segmentation_data$Smart.home.devices == "Yes", 1, 0)
segmentation_data$Owns.gaming.console <- ifelse(segmentation_data$Owns.gaming.console == "Yes", 1, 0)
# Scale numeric variables
segmentation_data_scaled <- scale(segmentation_data[, -which(names(segmentation_data) == "Annual.Household.income")])
# Apply K-means clustering
set.seed(123)
kmeans_result <- kmeans(segmentation_data_scaled, centers = 10)
# Add cluster labels to the dataset
segmentation_data$KMeans_Cluster <- kmeans_result$cluster
# Visualize K-means clustering results
fviz_cluster(kmeans_result, data = segmentation_data_scaled, geom = "point", ellipse = TRUE) +
labs(title = "K-means Clustering Results")
#write.csv(data, "Customer_Data_With_Clusters.csv", row.names = FALSE)
# Compute distance matrix
dist_matrix <- dist(segmentation_data_scaled, method = "euclidean")
# Apply hierarchical clustering
hierarchical_result <- hclust(dist_matrix, method = "ward.D2")
# Cut the tree into 4 clusters
segmentation_data$Hierarchical_Cluster <- cutree(hierarchical_result, k = 4)
# Visualize dendrogram
plot(hierarchical_result, main = "Dendrogram for Hierarchical Clustering", xlab = "", sub = "", cex = 0.6)
# Highlight clusters in dendrogram
rect.hclust(hierarchical_result, k = 4, border = 2:5)
# Summarize cluster profiles
kmeans_profile <- segmentation_data %>%
group_by(KMeans_Cluster) %>%
summarise(
Avg_Age = mean(Age, na.rm = TRUE),
Avg_Streaming_Hours = mean(Streaming.hours, na.rm = TRUE),
Avg_Income = mean(as.numeric(gsub("[$,]", "", Annual.Household.income)), na.rm = TRUE),
Smart_Device_Adoption = mean(Owns.smart.device, na.rm = TRUE),
Streaming_Addon_Adoption = mean(Video.streaming.add.on, na.rm = TRUE)
)
print(kmeans_profile)
write.csv(data, "Customer_Data_With_Clusters.csv", row.names = FALSE)
# Example: Assuming 'Cluster_Label' column exists in your dataset
data <- data %>%
mutate(Custom_Segment = case_when(
Cluster_Label == 1 ~ "Tech-Savvy Young Adults",
Cluster_Label == 2 ~ "High-Income Families",
Cluster_Label == 3 ~ "Streaming Enthusiasts",
Cluster_Label == 4 ~ "Budget-Conscious Seniors",
Cluster_Label == 5 ~ "Frequent Streamers",
Cluster_Label == 6 ~ "Casual Gamers",
Cluster_Label == 7 ~ "Luxury Spenders",
Cluster_Label == 8 ~ "Middle-Income Suburbanites",
Cluster_Label == 9 ~ "Rural Pragmatists",
Cluster_Label == 10 ~ "Digital Minimalists",
TRUE ~ "Other"
))
colnames(data)
# Apply K-means clustering
set.seed(123)
kmeans_result <- kmeans(segmentation_data_scaled, centers = 10)
# Add cluster labels to the original dataset
data$Cluster_Label <- kmeans_result$cluster
# Add cluster labels to the dataset
segmentation_data$KMeans_Cluster <- kmeans_result$cluster
# Visualize K-means clustering results
fviz_cluster(kmeans_result, data = segmentation_data_scaled, geom = "point", ellipse = TRUE) +
labs(title = "K-means Clustering Results")
# Compute distance matrix
dist_matrix <- dist(segmentation_data_scaled, method = "euclidean")
# Apply hierarchical clustering
hierarchical_result <- hclust(dist_matrix, method = "ward.D2")
# Cut the tree into 4 clusters
data$Hierarchical_Cluster <- cutree(hierarchical_result, k = 10)
# Visualize dendrogram
plot(hierarchical_result, main = "Dendrogram for Hierarchical Clustering", xlab = "", sub = "", cex = 0.6)
# Highlight clusters in dendrogram
rect.hclust(hierarchical_result, k = 4, border = 2:5)
# Summarize cluster profiles
kmeans_profile <- segmentation_data %>%
group_by(KMeans_Cluster) %>%
summarise(
Avg_Age = mean(Age, na.rm = TRUE),
Avg_Streaming_Hours = mean(Streaming.hours, na.rm = TRUE),
Avg_Income = mean(as.numeric(gsub("[$,]", "", Annual.Household.income)), na.rm = TRUE),
Smart_Device_Adoption = mean(Owns.smart.device, na.rm = TRUE),
Streaming_Addon_Adoption = mean(Video.streaming.add.on, na.rm = TRUE)
)
print(kmeans_profile)
colnames(data)
# Example: Assuming 'Cluster_Label' column exists in your dataset
data <- data %>%
mutate(Custom_Segment = case_when(
Cluster_Label == 1 ~ "Tech-Savvy Young Adults",
Cluster_Label == 2 ~ "High-Income Families",
Cluster_Label == 3 ~ "Streaming Enthusiasts",
Cluster_Label == 4 ~ "Budget-Conscious Seniors",
Cluster_Label == 5 ~ "Frequent Streamers",
Cluster_Label == 6 ~ "Casual Gamers",
Cluster_Label == 7 ~ "Luxury Spenders",
Cluster_Label == 8 ~ "Middle-Income Suburbanites",
Cluster_Label == 9 ~ "Rural Pragmatists",
Cluster_Label == 10 ~ "Digital Minimalists",
TRUE ~ "Other"
))
head(data)
# Example: Assuming 'Cluster_Label' column exists in your dataset
data <- data %>%
mutate(Custom_Segment = case_when(
Cluster_Label == 1 ~ "Tech-Savvy Young Adults",
Cluster_Label == 2 ~ "High-Income Families",
Cluster_Label == 3 ~ "Streaming Enthusiasts",
Cluster_Label == 4 ~ "Budget-Conscious Seniors",
Cluster_Label == 5 ~ "Frequent Streamers",
Cluster_Label == 6 ~ "Casual Gamers",
Cluster_Label == 7 ~ "Luxury Spenders",
Cluster_Label == 8 ~ "Middle-Income Suburbanites",
Cluster_Label == 9 ~ "Rural Pragmatists",
Cluster_Label == 10 ~ "Digital Minimalists",
TRUE ~ "Other"
))
head(data)
# Load necessary libraries
library(dplyr)
library(cluster)
library(factoextra)
# Load the dataset (replace with your file path)
data <- read.csv("C:\\Users\\rickm\\Merrimack\\DSE5004 - Visual Data Exploration\\Assignments\\Segmentation & Profiling\\Data\\Customer_Dataset_Data.csv")
# Select relevant colmuns for clustering
selected_columns <- c("Age", "Gender", "Education.years", "Streaming.hours",
"Owns.smart.device", "Video.streaming.add.on",
"Smart.home.devices", "Owns.gaming.console", "Annual.Household.income")
segmentation_data <- data %>%
select(all_of(selected_columns))
# Convert categorical variables to numeric (e.g., Gender, Owns.smart.device)
segmentation_data$Gender <- ifelse(segmentation_data$Gender == "Male", 1, 0)
segmentation_data$Owns.smart.device <- ifelse(segmentation_data$Owns.smart.device == "Yes", 1, 0)
segmentation_data$Video.streaming.add.on <- ifelse(segmentation_data$Video.streaming.add.on == "Yes", 1, 0)
segmentation_data$Smart.home.devices <- ifelse(segmentation_data$Smart.home.devices == "Yes", 1, 0)
segmentation_data$Owns.gaming.console <- ifelse(segmentation_data$Owns.gaming.console == "Yes", 1, 0)
# Scale numeric variables
segmentation_data_scaled <- scale(segmentation_data[, -which(names(segmentation_data) == "Annual.Household.income")])
# Apply K-means clustering
set.seed(123)
kmeans_result <- kmeans(segmentation_data_scaled, centers = 10)
# Add cluster labels to the original dataset
data$Cluster_Label <- kmeans_result$cluster
# Add cluster labels to the dataset
segmentation_data$KMeans_Cluster <- kmeans_result$cluster
# Visualize K-means clustering results
fviz_cluster(kmeans_result, data = segmentation_data_scaled, geom = "point", ellipse = TRUE) +
labs(title = "K-means Clustering Results")
# Compute distance matrix
dist_matrix <- dist(segmentation_data_scaled, method = "euclidean")
# Apply hierarchical clustering
hierarchical_result <- hclust(dist_matrix, method = "ward.D2")
# Cut the tree into 4 clusters
data$Hierarchical_Cluster <- cutree(hierarchical_result, k = 10)
# Visualize dendrogram
plot(hierarchical_result, main = "Dendrogram for Hierarchical Clustering", xlab = "", sub = "", cex = 0.6)
# Highlight clusters in dendrogram
rect.hclust(hierarchical_result, k = 4, border = 2:5)
# Summarize cluster profiles
kmeans_profile <- segmentation_data %>%
group_by(KMeans_Cluster) %>%
summarise(
Avg_Age = mean(Age, na.rm = TRUE),
Avg_Streaming_Hours = mean(Streaming.hours, na.rm = TRUE),
Avg_Income = mean(as.numeric(gsub("[$,]", "", Annual.Household.income)), na.rm = TRUE),
Smart_Device_Adoption = mean(Owns.smart.device, na.rm = TRUE),
Streaming_Addon_Adoption = mean(Video.streaming.add.on, na.rm = TRUE)
)
print(kmeans_profile)
# Example: Assuming 'Cluster_Label' column exists in your dataset
data <- data %>%
mutate(Custom_Segment = case_when(
Cluster_Label == 1 ~ "Tech-Savvy Young Adults",
Cluster_Label == 2 ~ "High-Income Families",
Cluster_Label == 3 ~ "Streaming Enthusiasts",
Cluster_Label == 4 ~ "Budget-Conscious Seniors",
Cluster_Label == 5 ~ "Frequent Streamers",
Cluster_Label == 6 ~ "Casual Gamers",
Cluster_Label == 7 ~ "Luxury Spenders",
Cluster_Label == 8 ~ "Middle-Income Suburbanites",
Cluster_Label == 9 ~ "Rural Pragmatists",
Cluster_Label == 10 ~ "Digital Minimalists",
TRUE ~ "Other"
))
head(data)
write.csv(data, "Customer_Data_With_Custom_Segments.csv", row.names = FALSE)
getwd()
setwd("~/GitHub/Team-Sigma")
#install.packages("haven")
library(haven)
brfss_2017 <- read_xpt("LLCP2017.XPT")
head(brfss_2017)
colnames(brfss_2017)
