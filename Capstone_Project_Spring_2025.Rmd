---
title: "Team Sigma Capstone Spring 2025"
author: "Rick Morales, Stephanie Halsing, and Erika Torkildsen"
date: "`r Sys.Date()`"
output: pdf_document
---

Libraries and Data Read-in

Libraries are loaded in as a first step in preparing the data for analysis in order to run functions within each package. Library descriptions have been commented beside each command to provide supplementary information that helps new users understand the purpose behind using the packages specified in this study and for reproducibility. - Erika Torkildsen, Stephanie Halsing, and Rick Morales. Code below is self tested and tested on other machines successfully.
```{r}
library(haven) # data formatting
library(dplyr) # data transformation
library(mice) # imputation
library(factoextra) # plot eigenvalues/variances
library(tidyverse) # set of packages for tidying data
library(corrplot) # for correlelogram
library(caret) # classification and regression training
library(parameters) # processing statistical model parameters
library(see) # passes objects from parameters package
library(lmtest)  # assumption testing
library(ggplot2) # visualizations
library(car)  # variance inflation factor (VIF)
library(ordinalNet) # ordinal logistic regression
library(plyr) # set of tools for splitting up data
library(smotefamily) # generates synthetic samples to address class imbalance
library(MASS) # supports functions from Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002)
library(MLmetrics) # model evaluation metrics
library(ipred) # improve predictive models
library(rpart) # recursive partitioning and regression trees
library(pROC) # analyze ROC curves
library(randomForest) # classification and regression based on Breiman (2001) <doi:10.1023/A:1010933404324>
library(mice) # multiple imputations for multivariate missing data
library(tidyr) # tidy messy data
library(PRROC) # precision recall and ROC curves

brfss_2017 <- read_xpt("LLCP2017.XPT") #Reading in 2017 Behaviroal Risk Factor Surveillance System file from an export file

head(brfss_2017) #Viewing first six rows of dataset for data preview
```
Data Cleaning

Removal of Missing Columns

Variables with more than 10% missing values are removed from the data in order to eliminate the potential for biased results. The remainiung data may not be representative of the entire population and lead to skewed results. - Stephanie Halsing and Erika Torkildsen. Code below is self tested and tested on other machines successfully.
```{r}
# The select() function is used by dplyr to deselect columns specified by their locations (indices) in the data frame. c() combines selections and 
brfss_2017_cleaned <- brfss_2017 %>%
    dplyr::select(-c(2:6), -c(10:27), -31, -37, -40, -45, -c(53:57), -c(62:64), -72, -c(80:82), -85, -c(87:89), -c(97:102), -106, -108, -110, -c(112:246), ... = -c(255:258), -c(326:327), -c(330:337), -c(340:345), -c(356:357))

# Check that select() removed columns successfully
head(brfss_2017_cleaned)
```
Renaming Variable Names

Select variable names are renamed for clarity and understanding and to eliminate confusion. - Erika Torkildsen. Code below is self tested and tested on other machines successfully.
```{r}
# Names() takes in variable indices and specified new variable names
names(brfss_2017_cleaned)[1] <- "STATE"
names(brfss_2017_cleaned)[4] <- "PSU"
names(brfss_2017_cleaned)[8] <- "HLTHPLN"
names(brfss_2017_cleaned)[9] <- "PERSDOC"
names(brfss_2017_cleaned)[11] <- "CHECKUP"
names(brfss_2017_cleaned)[12] <- "BPHIGH"
names(brfss_2017_cleaned)[13] <- "CHOLCHK"
names(brfss_2017_cleaned)[14] <- "TOLDHI"
names(brfss_2017_cleaned)[15] <- "CVDINFR"
names(brfss_2017_cleaned)[16] <- "CVDCRHD"
names(brfss_2017_cleaned)[17] <- "CVDSTRK"
names(brfss_2017_cleaned)[18] <- "ASTHMA"
names(brfss_2017_cleaned)[21] <- "CHCCOPD"
names(brfss_2017_cleaned)[22] <- "HAVARTH"
names(brfss_2017_cleaned)[23] <- "ADDEPEV"
names(brfss_2017_cleaned)[25] <- "DIABETES"
names(brfss_2017_cleaned)[29] <- "RENTHOME"
names(brfss_2017_cleaned)[30] <- "VETERAN"
names(brfss_2017_cleaned)[31] <- "EMPLOYED"
names(brfss_2017_cleaned)[33] <- "INCOME"
names(brfss_2017_cleaned)[35] <- "WEIGHT"
names(brfss_2017_cleaned)[36] <- "HEIGHT"
names(brfss_2017_cleaned)[44] <- "USENOW"
names(brfss_2017_cleaned)[47] <- "FRUIT"
names(brfss_2017_cleaned)[48] <- "FRUIT_JUICE"
names(brfss_2017_cleaned)[49] <- "GREEN_VEG"
names(brfss_2017_cleaned)[50] <- "FRENCHF"
names(brfss_2017_cleaned)[51] <- "POTATO"
names(brfss_2017_cleaned)[52] <- "OTHER_VEG"
names(brfss_2017_cleaned)[53] <- "EXERCISE_ANY"
names(brfss_2017_cleaned)[56] <- "FLUSHOT"
names(brfss_2017_cleaned)[57] <- "PNEUMONIA_VAC"
names(brfss_2017_cleaned)[58] <- "HIVTEST"
names(brfss_2017_cleaned)[59] <- "HIVRISK"
names(brfss_2017_cleaned)[63] <- "STSTR"
names(brfss_2017_cleaned)[64] <- "STRATUM_WGHT"
names(brfss_2017_cleaned)[65] <- "RAWRAKE"
names(brfss_2017_cleaned)[66] <- "WT2RAKE"
names(brfss_2017_cleaned)[67] <- "IMPRACE"
names(brfss_2017_cleaned)[68] <- "DUALUSE"
names(brfss_2017_cleaned)[69] <- "DUALCOR"
names(brfss_2017_cleaned)[70] <- "LLCPWT2"
names(brfss_2017_cleaned)[71] <- "LLCPWT"
names(brfss_2017_cleaned)[72] <- "RFHLTH"
names(brfss_2017_cleaned)[73] <- "PHYS_COMP"
names(brfss_2017_cleaned)[74] <- "MENT_COMP"
names(brfss_2017_cleaned)[75] <- "HCVU651"
names(brfss_2017_cleaned)[76] <- "RFHYPES"
names(brfss_2017_cleaned)[77] <- "CHOLCH1"
names(brfss_2017_cleaned)[78] <- "HICHOL_COMP"
names(brfss_2017_cleaned)[79] <- "MI_CHD"
names(brfss_2017_cleaned)[80] <- "ASTHMA_LIFE"
names(brfss_2017_cleaned)[81] <- "ASTHMA_NOW_COMP"
names(brfss_2017_cleaned)[82] <- "ASTHMA_COMP"
names(brfss_2017_cleaned)[83] <- "ARTHRITIS_DIAGNOS"
names(brfss_2017_cleaned)[84] <- "LIMIT_ACTIVITY"
names(brfss_2017_cleaned)[85] <- "LIMIT_WORK"
names(brfss_2017_cleaned)[86] <- "LIMIT_SOCIAL"
names(brfss_2017_cleaned)[87] <- "PRACE_COMP"
names(brfss_2017_cleaned)[88] <- "MULTI_RACE"
names(brfss_2017_cleaned)[89] <- "HISPANIC_COMP"
names(brfss_2017_cleaned)[90] <- "RACE_COMP"
names(brfss_2017_cleaned)[91] <- "RACEG21"
names(brfss_2017_cleaned)[92] <- "RACEGR3"
names(brfss_2017_cleaned)[93] <- "RACE_G1"
names(brfss_2017_cleaned)[94] <- "AGEG5YR"
names(brfss_2017_cleaned)[95] <- "AGE65YR"
names(brfss_2017_cleaned)[96] <- "AGE80"
names(brfss_2017_cleaned)[97] <- "AGE_G"
names(brfss_2017_cleaned)[98] <- "HT_COMP_IN"
names(brfss_2017_cleaned)[99] <- "HT_COMP_M"
names(brfss_2017_cleaned)[100] <- "WT_COMP_KILO"
names(brfss_2017_cleaned)[101] <- "BMI"
names(brfss_2017_cleaned)[102] <- "BMI_CAT"
names(brfss_2017_cleaned)[103] <- "BMI_OVER"
names(brfss_2017_cleaned)[104] <- "CHILD_COMP"
names(brfss_2017_cleaned)[105] <- "EDUCA_COMP"
names(brfss_2017_cleaned)[106] <- "INCOME_COMP"
names(brfss_2017_cleaned)[107] <- "SMOKER_COMP"
names(brfss_2017_cleaned)[108] <- "SMOKENOW_COMP"
names(brfss_2017_cleaned)[109] <- "ECIG_COMP"
names(brfss_2017_cleaned)[110] <- "ECIG_NOW_COMP"
names(brfss_2017_cleaned)[111] <- "DRINKANY_30"
names(brfss_2017_cleaned)[112] <- "DROCDY3"
names(brfss_2017_cleaned)[113] <- "BINGE_COMP"
names(brfss_2017_cleaned)[114] <- "WEEKLY_DRINK"
names(brfss_2017_cleaned)[115] <- "HEAVY_DRINK"
names(brfss_2017_cleaned)[116] <- "FRUITJUICE_COMP"
names(brfss_2017_cleaned)[117] <- "FRUITJUICE_COMP2"
names(brfss_2017_cleaned)[118] <- "GREEN_VEG_COMP"
names(brfss_2017_cleaned)[119] <- "FRENCHF_COMP"
names(brfss_2017_cleaned)[120] <- "POTATO_COMP"
names(brfss_2017_cleaned)[121] <- "OTHER_VEG_COMP"
names(brfss_2017_cleaned)[122] <- "MISSING_FRUIT"
names(brfss_2017_cleaned)[123] <- "MISSING_VEG"
names(brfss_2017_cleaned)[124] <- "MISS_ANY_FRUIT"
names(brfss_2017_cleaned)[125] <- "MISS_ANY_VEG"
names(brfss_2017_cleaned)[126] <- "TOTAL_FRUIT"
names(brfss_2017_cleaned)[127] <- "TOTAL_VEG"
names(brfss_2017_cleaned)[128] <- "ATLEAST1_FRUIT"
names(brfss_2017_cleaned)[129] <- "ATLEAST1_VEG"
names(brfss_2017_cleaned)[130] <- "MORE16_FRUIT"
names(brfss_2017_cleaned)[131] <- "MORE23_VEG"
names(brfss_2017_cleaned)[132] <- "FRUIT_EXCLUDE"
names(brfss_2017_cleaned)[133] <- "VEG_EXCLUDE"
names(brfss_2017_cleaned)[134] <- "LEISURE_COMP"
names(brfss_2017_cleaned)[135] <- "MAX_OXYGEN"
names(brfss_2017_cleaned)[136] <- "FUNC_CAPACITY"
names(brfss_2017_cleaned)[137] <- "STRENGTH_FREQ"
names(brfss_2017_cleaned)[138] <- "MISSING_PHYS"
names(brfss_2017_cleaned)[139] <- "PHYS_CATEGORY"
names(brfss_2017_cleaned)[140] <- "PHYS_IDX"
names(brfss_2017_cleaned)[141] <- "150_PHYS_COMP"
names(brfss_2017_cleaned)[142] <- "300_PHYS_COMP"
names(brfss_2017_cleaned)[143] <- "300_PHYS2_COMP"
names(brfss_2017_cleaned)[144] <- "MUSCLE_RECOMMEND"
names(brfss_2017_cleaned)[145] <- "AEROBIC_STRENGTH"
names(brfss_2017_cleaned)[146] <- "AEROBIC_STRENGTH2"
names(brfss_2017_cleaned)[147] <- "SEATBELT_COMP"
names(brfss_2017_cleaned)[148] <- "SEATBELT_COMP2"
names(brfss_2017_cleaned)[149] <- "AIDSTEST_COMP"

# Check that names() renamed columns successfully
head(brfss_2017_cleaned)
```
Additional Column Removal

Additional columns are removed due to irrelevance to the study and to account for computed variables that require source variables to be dropped. Values computed from the original data that contain NAs are removed. Stephanie Halsing and Erika Torkildsen. Code below is self tested and tested on other machines successfully.
```{r}
# Removal of additional calculated and irrelevant variables
brfss_2017_cleaned <- brfss_2017_cleaned %>%
  dplyr::select(-c(35, 36, 43, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 106, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149))

# Computed categories given a value of 9 contain NAs and are removed
brfss_2017_cleaned$ECIG_COMP[brfss_2017_cleaned$ECIG_COMP == 9] <- NA
brfss_2017_cleaned$SMOKER_COMP[brfss_2017_cleaned$SMOKER_COMP == 9] <- NA

# Remove SEQNO since it's a unique identifier
brfss_2017_cleaned <- brfss_2017_cleaned %>%
    dplyr::select(-c(SEQNO))

# Filter for only asthmatics
#brfss_2017_cleaned <- brfss_2017_cleaned %>%
 # filter(ASTHMA == 1)

# Ensure unique column names
colnames(brfss_2017_cleaned) <- make.names(colnames(brfss_2017_cleaned), unique = TRUE)
```
Exploratory Data Analysis

Target Leakage Risks

Checking correlation of variables and computing the variance inflation factor to view linear association and check for multicollinearity. Variance inflation factor checks for inflation of variance in a regression coefficient for correlation between predictors. Stephanie Halsig. Code below is self tested and tested on other machines successfully.
```{r}
# Correlation for all variables
round(cor(brfss_2017_cleaned), digits = 2)

# Create model for VIF
vif_model <- lm(ECIG_COMP ~ AGE80 + BMI + SMOKER_COMP + STATE + ADDEPEV + ALCDAY5 + CHCCOPD + CHECKUP + EDUCA + GENHLTH + HLTHPLN + INCOME + SEX, data = brfss_2017_cleaned)

# Calculating VIF
vif_values <- vif(vif_model)
vif_values

# Visualizing the model
plot(vif_model, which = 1, main = "Model Fit")

# Visualizing VIF
barplot(vif_values, las = 2, col = "skyblue", main = "Variance Inflation Factor (VIF)")

# Creating a correlation matrix
cor_matrix <- cor(brfss_2017_cleaned[c('AGE80', 'BMI', 'SMOKER_COMP', 'STATE', 'ADDEPEV', 'ALCDAY5',
                    'CHCCOPD', 'CHECKUP', 'EDUCA', 'GENHLTH', 'HLTHPLN', 'INCOME', 'SEX')])

# Visualizing the correlation matrix
image(cor_matrix, main = "Correlation Matrix", col = colorRampPalette(c("blue", "white", "red"))(20))
```
Visualize Relationships

Visualizations are made to explore patterns in data relevant to our research question prior to pre-processing. ggplot() is used for visualizations with inputs to display count percentages, labels, titles, and legends. Erika Torkildsen and Rick Morales. Code below is self tested and tested on other machines successfully.
```{r}
# Visualizing E-cigarette smoking status (binary)
EDA_1 <- ggplot(brfss_2017_cleaned, aes(x = as.factor(ECIG_COMP), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ECIG_COMP),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Every day', 'Some days', "Former Smoker", "Never smoked")) + 
    labs(title = "E-Cigarette Usage Among All Adults Surveyed in 2017", x = "E-Cigarette Usage", y = "Surveyed Population %", fill = "E-Cigarette Usage")

EDA_1
```
```{r}
# Visualizing Conventional Smoking Status
EDA_2 <- ggplot(brfss_2017_cleaned, aes(x = as.factor(SMOKER_COMP), 
                      y = prop.table(stat(count)),
                      fill = as.factor(SMOKER_COMP),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Every day', 'Some days', "Former Smoker", "Never smoked")) + 
    labs(title = "Conventional Smoking Among All Adults Surveyed in 2017", x = "Smoking Status", y = "Surveyed Population %", fill = "Smoking Status")

EDA_2
```
```{r}
#Visualizing Asthma Status
EDA_3 <- ggplot(brfss_2017_cleaned, aes(x = as.factor(ASTHMA), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ASTHMA),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Yes', 'No', "Unsure", "Refused")) + 
    labs(title = "Asthma Diagnosis Among All Adults Surveyed in 2017", x = "Asthma Diagnosis", y = "Surveyed Population %", fill = "Diagnosis")

EDA_3
```
```{r}
# Visualizing Asthma and Vaping Status
EDA_4 <- ggplot(brfss_2017_cleaned, aes(x = as.factor(ECIG_COMP), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ASTHMA))) + 
    geom_bar(position = "dodge") +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Every day', 'Some days', "Former User", "Never Used")) + 
    labs(title = "E-Cigarette Usage Among Asthmatics in 2017", x = "E-Cigarette Usage", y = "Surveyed Population %", fill = "Asthma Diagnosis")

EDA_4
```
```{r}
# Visualizing Conventional Smoking and Vaping Status
EDA_5 <- ggplot(brfss_2017_cleaned, aes(x = as.factor(ECIG_COMP), 
                      y = prop.table(stat(count)),
                      fill = as.factor(SMOKER_COMP))) + 
    geom_bar(position = "dodge") +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Every day', 'Some days', "Former User", "Never Used")) + 
    labs(title = "E-Cigarette Usage Among Conventional Smokers in 2017", x = "E-Cigarette Usage", y = "Surveyed Population %", fill = "Smoking Status")

EDA_5
```
```{r}
# Visualizing Conventional Smoking Status and Asthma Status
EDA_6 <- ggplot(brfss_2017_cleaned, aes(x = as.factor(SMOKER_COMP), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ASTHMA))) + 
    geom_bar(position = "dodge") +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Every day', 'Some days', "Former Smoker", "Never smoked")) + 
    labs(title = "Conventional Smoking Among Asthmatics in 2017", x = "Smoking Status", y = "Surveyed Population %", fill = "Asthma Status")

EDA_6
```
Pre-processing and Feature Engineering for Binary Target

Data is prepped for the three models to be tested for the binary e-cigarette smoking status variable. A new data frame is created from the cleaned dataset for each of three test cases (filtered on asthmatics with history of vaping, asthmatics with history of conventional smoking and vaping, and an unfiltered test case). Columns are factored and createDataPartition() splits the data into training and test sets while making making the alogorithm aware of the target variable. Rick Morales. Code below is self tested and tested on other machines successfully.

Train-Test Split for Binary E-Cigarette Usage Variable (asthmatics with history of vaping)
```{r}
# Binary Variable Creation
df <- brfss_2017_cleaned

# Filter on adult asthmatics with history of vaping
  df <- brfss_2017_cleaned %>%
      filter(
          ECIG_COMP != 4,
          ASTHMA == 1
          )

# factoring target variable and ensuring data compatibility
df$ECIG_BINARY <- ifelse(df$ECIG_COMP == 1 | df$ECIG_COMP == 2, 1, 0)
df$ECIG_COMP <- as.factor(df$ECIG_COMP)
df$ECIG_BINARY <- as.factor(df$ECIG_BINARY)
df$STATE <- as.numeric(as.character(df$STATE))

#train-test split with 1 partition, 70% of data in training, and results not stored in list
set.seed(1)
split_bin <- createDataPartition(df$ECIG_BINARY, times = 1, p = .7, list = FALSE)
train_bin <- df[split_bin, ]
test_bin <- df[-split_bin, ]
```

Train-Test Split (asthmatics with history of vaping and smoking)
```{r}
# Binary Variable Creation
df_2 <- brfss_2017_cleaned

# Filter on adult asthmatics with history of vaping and smoking
  df_2 <- brfss_2017_cleaned %>%
      filter(
          ECIG_COMP != 4,
          ASTHMA == 1,
          SMOKER_COMP != 4
          )

# factoring target variable and ensuring data compatibility
df_2$ECIG_BINARY <- ifelse(df_2$ECIG_COMP == 1 | df_2$ECIG_COMP == 2, 1, 0)
df_2$ECIG_COMP <- as.factor(df_2$ECIG_COMP)
df_2$ECIG_BINARY <- as.factor(df_2$ECIG_BINARY)
df_2$STATE <- as.numeric(as.character(df_2$STATE))

#train-test split with 1 partition, 70% of data in training, and results not stored in list
set.seed(2)
split_bin_2 <- createDataPartition(df_2$ECIG_BINARY, times = 1, p = .7, list = FALSE)
train_bin_2 <- df_2[split_bin_2, ]
test_bin_2 <- df_2[-split_bin_2, ]
```

Train_test Split (all adult asthmatics)
```{r}
# Binary Variable Creation
df_nf <- brfss_2017_cleaned

# Filter on adult asthmatics
df_nf <- brfss_2017_cleaned %>%
    filter(ASTHMA == 1)

# factoring target variable and ensuring data compatibility
df_nf$ECIG_BINARY <- ifelse(df_nf$ECIG_COMP == 1 | df_nf$ECIG_COMP == 2, 1, 0)
df_nf$ECIG_COMP <- as.factor(df_nf$ECIG_COMP)
df_nf$ECIG_BINARY <- as.factor(df_nf$ECIG_BINARY)
df_nf$STATE <- as.numeric(as.character(df_nf$STATE))

#train-test split with 1 partition, 70% of data in training, and results not stored in list
set.seed(3)
split_bin_nf <- createDataPartition(df_nf$ECIG_BINARY, times = 1, p = .7, list = FALSE)
train_bin_nf <- df_nf[split_bin_nf, ]
test_bin_nf <- df_nf[-split_bin_nf, ]
```

Imputation of Missing Values for Binary Target

Survey data assumes missing values are classified as missing at random. Multiple imputation by chained equations is selected and computed from the mice() package as is best for survey data. Method chosen in each imputation is cart (imputation by classification and regression trees). Erika Torkildsen. Code below is self tested and tested on other machines successfully.

Imputation for Model Filtered on Asthmatics with History of Vaping
```{r}
# View available methods within mice()
methods(mice)

# Calculate imputed values with mice() using cart with 1 multiple imputation and 1 iteration. Reproducible.
imp_bin <- mice(train_bin, method = "cart", m = 1, maxit = 1, seed = 4)

# Complete training data with imputed values
train_bin <- complete(imp_bin)
```

Imputation for Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
# View available methods within mice()
methods(mice)

# Calculate imputed values with mice() using cart with 1 multiple imputation and 1 iteration. Reproducible.
imp_bin_2 <- mice(train_bin_2, method = "cart", m = 1, maxit = 1, seed = 5)

# Complete training data with imputed values
train_bin_2 <- complete(imp_bin_2)
```

Imputation for Model Filtered on Asthmatics
```{r}
# View available methods within mice()
methods(mice)

# Calculate imputed values with mice() using cart with 1 multiple imputation and 1 iteration. Reproducible.
imp_bin_nf <- mice(train_bin_nf, method = "cart", m = 1, maxit = 1, seed = 6)

# Complete training data with imputed values
train_bin_nf <- complete(imp_bin_nf)
```
PCA and K-Means Clustering for Unsupervised Learning

Principal component analysis is computed for each of the three models to be tested for the binary e-cigarette smoking status variable. preProcess() is used for transforming the training sets. It takes in predictors as inputs and the pre-processing method specified. Rick Morales. Code below is self tested and tested on other machines successfully.

PCA for Model Filtered on Asthmatics with History of Vaping
```{r}
preprocessed_data <- preProcess(train_bin[, c('ASTHMA', 'AGE80', 'BMI', 'SMOKER_COMP', 'STATE', 'ADDEPEV', 'ALCDAY5',
                    'CHCCOPD', 'CHECKUP', 'EDUCA', 'GENHLTH', 'HLTHPLN', 'INCOME', 'SEX')], method = "range")
transformed_data <- predict(preprocessed_data, train_bin)

# checks for numeric values
numerical_data <- transformed_data %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(where(~ var(.x, na.rm = TRUE) != 0))

numerical_data <- numerical_data %>%
  filter_all(all_vars(is.finite(.))) %>%
  na.omit()

# pca computation for list of components and proportion of variance
pca <- princomp(numerical_data)
summary(pca)
pca$loadings[, 1:2]

# plots eigenvalues/variances against dimensions
fviz_eig(pca, addlabels = TRUE)
```
PCA for Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
preprocessed_data_2 <- preProcess(train_bin_2[, c("ASTHMA", 'AGE80', 'BMI', 'SMOKER_COMP', 'STATE', 'ADDEPEV', 'ALCDAY5',
                    'CHCCOPD', 'CHECKUP', 'EDUCA', 'GENHLTH', 'HLTHPLN', 'INCOME', 'SEX')], method = "range")
transformed_data_2 <- predict(preprocessed_data_2, train_bin_2)

# checks for numeric values
numerical_data_2 <- transformed_data_2 %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(where(~ var(.x, na.rm = TRUE) != 0))

numerical_data_2 <- numerical_data_2 %>%
  filter_all(all_vars(is.finite(.))) %>%
  na.omit()

# pca computation for list of components and proportion of variance
pca_2 <- princomp(numerical_data_2)
summary(pca_2)
pca_2$loadings[, 1:2]

# plots eigenvalues/variances against dimensions
fviz_eig(pca_2, addlabels = TRUE)
```
PCA for Model Filtered on Asthmatics
```{r}
preprocessed_data_nf <- preProcess(train_bin_nf[, c("ASTHMA", 'AGE80', 'BMI', 'SMOKER_COMP', 'STATE', 'ADDEPEV', 'ALCDAY5',
                    'CHCCOPD', 'CHECKUP', 'EDUCA', 'GENHLTH', 'HLTHPLN', 'INCOME', 'SEX')], method = "range")
transformed_data_nf <- predict(preprocessed_data_nf, train_bin_nf)

# checks for numeric values
numerical_data_nf <- transformed_data_nf %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(where(~ var(.x, na.rm = TRUE) != 0))

numerical_data_nf <- numerical_data_nf %>%
  filter_all(all_vars(is.finite(.))) %>%
  na.omit()

# pca computation for list of components and proportion of variance
pca_nf <- princomp(numerical_data_nf)
summary(pca_nf)
pca_nf$loadings[, 1:2]

# plots eigenvalues/variances against dimensions
fviz_eig(pca_nf, addlabels = TRUE)
```
Optimal number of clusters computation, computed for each model. 'wss' represents total within sum of squares. Erika Torkildsen. Code below is self tested and tested on other machines successfully.

Optimal Clusters Computation for Model Filtered on Asthmatics with History of Vaping
```{r}
fviz_nbclust(train_bin, kmeans, method = "wss")
```
Optimal Clusters Computation for Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
fviz_nbclust(train_bin_2, kmeans, method = "wss")
```
```{r}
# Machines ran out of memory and were unable to compute optimal cluster for the third model.Several attempts to address this failed.
```

K-means clustering is computed for each model to partition the data into clusters. kmeans() is used with transformed data as an input. Centers represents the number of specified clusters. nstart represents how many random sets should be chosen. fviz_cluster() visualizes results. Erika Torkildsen and Rick Morales. Code below is self tested and tested on other machines successfully.

K-Means for Model Filtered on Asthmatics with History of Vaping
```{r}
# ensuring data is compatible with K-means algorithm
kmeans_data <- transformed_data %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(where(~ var(.x, na.rm = TRUE) != 0)) %>%
  na.omit()

# K-means computation with 3 clusters and 25 random sets
set.seed(123)
kmeans_result <- kmeans(kmeans_data, centers = 3, nstart = 25)

# visualize cluster results, three colors specified, data point type, size, and plot theme selected
fviz_cluster(kmeans_result, 
             data = kmeans_data,
             palette = c("red", "blue", "green"),
             geom = "point",
             pointsize = 1.2,
             ellipse.type = "t",
             show.clust.cent = TRUE,
             ggtheme = theme_minimal()) +
  labs(title = "K-Means Clustering of BRFSS Data (k = 3)")

kmeans_data$Cluster <- as.factor(kmeans_result$cluster)
```
K-Means for Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
# ensuring data is compatible with K-means algorithm
kmeans_data_2 <- transformed_data_2 %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(where(~ var(.x, na.rm = TRUE) != 0)) %>%
  na.omit()

# K-means computation with 3 clusters and 25 random sets
set.seed(456)
kmeans_result_2 <- kmeans(kmeans_data_2, centers = 3, nstart = 25)

# visualize cluster results, three colors specified, data point type, size, and plot theme selected
fviz_cluster(kmeans_result_2, 
             data = kmeans_data_2,
             palette = c("red", "blue", "green"),
             geom = "point",
             pointsize = 1.2,
             ellipse.type = "t",
             show.clust.cent = TRUE,
             ggtheme = theme_minimal()) +
  labs(title = "K-Means Clustering of BRFSS Data (k = 3)")

kmeans_data_2$Cluster <- as.factor(kmeans_result_2$cluster)
```
K-Means for Model Filtered on Asthmatics
```{r}
# ensuring data is compatible with K-means algorithm
kmeans_data_nf <- transformed_data_nf %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(where(~ var(.x, na.rm = TRUE) != 0)) %>%
  na.omit()

# K-means computation with 3 clusters and 25 random sets
set.seed(789)
kmeans_result_nf <- kmeans(kmeans_data_nf, centers = 3, nstart = 25)

# visualize cluster results, three colors specified, data point type, size, and plot theme selected
fviz_cluster(kmeans_result_nf, 
             data = kmeans_data_nf,
             palette = c("red", "blue", "green"),
             geom = "point",
             pointsize = 1.2,
             ellipse.type = "t",
             show.clust.cent = TRUE,
             ggtheme = theme_minimal()) +
  labs(title = "K-Means Clustering of BRFSS Data (k = 3)")

kmeans_data_nf$Cluster <- as.factor(kmeans_result_nf$cluster)
```
Class Imbalance Checks

A check is made to test for class imbalance through the use of bar plots with ggplot2, examining proportions of predictor categories in the binary target. Erika Torkildsen. Code below is self tested and tested on other machines successfully.

Class Imbalance Check for Model Filtered on Asthmatics with History of Vaping
```{r}
p1 <- ggplot(train_bin, aes(x = as.factor(ECIG_BINARY), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ECIG_BINARY),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Never Used', 'Used')) + 
    labs(title = "E-Cigarette Smoking Status Among Those with History of Asthma in 2017", x = "E-Cig Smoking Status", y = "Surveyed Population %", fill = "E-Cig Smoking Status")

p1
```
Class Imbalance Check for Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
p2 <- ggplot(train_bin_2, aes(x = as.factor(ECIG_BINARY), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ECIG_BINARY),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Never Used', 'Used')) + 
    labs(title = "E-Cigarette Usage in 2017 in Adults with History of Asthma and Smoking", x = "E-Cig Smoking Status", y = "Surveyed Population %", fill = "E-Cig Smoking Status")

p2
```
Class Imbalance Check for Model Filtered on Asthmatics
```{r}
p3 <- ggplot(train_bin_nf, aes(x = as.factor(ECIG_BINARY), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ECIG_BINARY),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Never Used', 'Used')) + 
    labs(title = "E-Cigarette Smoking Status Among Those with History of Asthma in 2017", x = "E-Cig Smoking Status", y = "Surveyed Population %", fill = "E-Cig Smoking Status")

p3
```
Predictor variables are specified, checked for numeric state and a predictor matrix created for the final modeling data frame. Stephanie Halsing. Code below is self tested and tested on other machines successfully.

Predictor and Target Setup for Model Filtered on Asthmatics with History of Vaping
```{r}
predictors <- c("ASTHMA", "AGE80", "BMI", "SMOKER_COMP", "STATE", "ADDEPEV",
                "CHCCOPD", "CHECKUP", "GENHLTH", "HLTHPLN",
                "EDUCA", "INCOME", "SEX", "ALCDAY5")

# Convert factor predictors to numeric if needed
train_bin[predictors] <- lapply(train_bin[predictors], function(col) {
  if (is.factor(col)) suppressWarnings(as.numeric(as.character(col))) else col
})

train_bin <- train_bin %>%
  filter(if_any(all_of(predictors), ~ !is.na(.) & is.finite(.)))

# Create predictor matrix
x <- data.matrix(train_bin[, predictors])

# Target
y <- train_bin$ECIG_BINARY

# Final modeling data frame
data_rf <- data.frame(x, y)
```

Predictor and Target Setup for Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
# Convert factor predictors to numeric if needed
train_bin_2[predictors] <- lapply(train_bin_2[predictors], function(col) {
  if (is.factor(col)) suppressWarnings(as.numeric(as.character(col))) else col
})

train_bin_2 <- train_bin_2 %>%
  filter(if_any(all_of(predictors), ~ !is.na(.) & is.finite(.)))

# Create predictor matrix
x_2 <- data.matrix(train_bin_2[, predictors])

# Target
y_2 <- train_bin_2$ECIG_BINARY

# Final modeling data frame
data_rf_2 <- data.frame(x_2, y_2)
```

Predictor and Target Setup for Model Filtered on Asthmatics
```{r}
# Convert factor predictors to numeric if needed
train_bin_nf[predictors] <- lapply(train_bin_nf[predictors], function(col) {
  if (is.factor(col)) suppressWarnings(as.numeric(as.character(col))) else col
})

train_bin_nf <- train_bin_nf %>%
  filter(if_any(all_of(predictors), ~ !is.na(.) & is.finite(.)))

# Create predictor matrix
x_nf <- data.matrix(train_bin_nf[, predictors])

# Target
y_nf <- train_bin_nf$ECIG_BINARY

# Final modeling data frame
data_rf_nf <- data.frame(x_nf, y_nf)
```

Random Forest Models (Supervised learning)

A random forest model is implemented for each test case (filtered asthmatics, filtered asthmatics and smokers, and unfiltered) using multiple decision trees to make predictions. Erika Torkildsen and Rick Morales. Code below is self tested and tested on other machines successfully.

RF Model Filtered on Asthmatics with History of Vaping
```{r}
# Convert outcome to factor for classification
train_bin$ECIG_BINARY <- factor(ifelse(train_bin$ECIG_COMP == 1 | train_bin$ECIG_COMP == 2, 1, 0))

# Train Random Forest model
set.seed(10)
rf_model <- randomForest(
  x = train_bin[, predictors],
  y = train_bin$ECIG_BINARY,
  ntree = 1000,          # number of trees
  mtry = 2,              # number of variables per split (can tune)
  nodesize = 4,          # minimum size of terminal nodes
  importance = TRUE      # importance of predictors is assessed
)

# Check model summary
print(rf_model)
```
RF Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
# Convert outcome to factor for classification
train_bin_2$ECIG_BINARY <- factor(ifelse(train_bin_2$ECIG_COMP == 1 | train_bin_2$ECIG_COMP == 2, 1, 0))

# Train Random Forest model
set.seed(11)
rf_model_2 <- randomForest(
  x = train_bin_2[, predictors],
  y = train_bin_2$ECIG_BINARY,
  ntree = 1000,          # number of trees
  mtry = 2,              # number of variables per split (can tune)
  nodesize = 4,          # minimum size of terminal nodes
  importance = TRUE      # importance of predictors is assessed
)

# Check model summary
print(rf_model_2)
```
RF Model Filtered on Asthmatics
```{r}
# Convert outcome to factor for classification
train_bin_nf$ECIG_BINARY <- factor(ifelse(train_bin_nf$ECIG_COMP == 1 | train_bin_nf$ECIG_COMP == 2, 1, 0))

# Train Random Forest model
set.seed(12)
rf_model_nf <- randomForest(
  x = train_bin_nf[, predictors],
  y = train_bin_nf$ECIG_BINARY,
  ntree = 1000,          # number of trees
  mtry = 2,              # number of variables per split (can tune)
  nodesize = 4,          # minimum size of terminal nodes
  importance = TRUE      # importance of predictors is assessed
)

# Check model summary
print(rf_model_nf)
```
Receiver Operating Characteristic (ROC) Curves and Areas Under the Curve (AUC)

For each computed random forest a corresponding ROC curve is computed from roc() with predicted probabilities as an input. The AUC (area under the curve) value is calculated from the roc curve. Rick Morales and Erika Torkildsen. Code below is self tested and tested on other machines successfully.

RF Model Filtered on Asthmatics with History of Vaping
```{r}
# Predicted probabilities
rf_probs <- predict(rf_model, type = "prob")[, 2]  # probability of class "1"

# ROC curve
rf_roc <- roc(as.numeric(as.character(train_bin$ECIG_BINARY)), rf_probs)

# AUC value
auc(rf_roc)

# Plot ROC
a <- plot(rf_roc, col = "blue", lwd = 2, main = "Random Forest ROC Curve", print.auc = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")
```
RF Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
# Predicted probabilities
rf_probs_2 <- predict(rf_model_2, type = "prob")[, 2]  # probability of class "1"

# ROC curve
rf_roc_2 <- roc(as.numeric(as.character(train_bin_2$ECIG_BINARY)), rf_probs_2, col = "green")

# AUC value
auc(rf_roc_2)

# Plot ROC
b <- plot(rf_roc_2, col = "forestgreen", lwd = 2, main = "Random Forest ROC Curve", print.auc = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")
```
RF Model Filtered on Asthmatics
```{r}
# Predicted probabilities
rf_probs_nf <- predict(rf_model_nf, type = "prob")[, 2]  # probability of class "1"

# ROC curve
rf_roc_nf <- roc(as.numeric(as.character(train_bin_nf$ECIG_BINARY)), rf_probs_nf)

# AUC
auc(rf_roc_nf)

# Plot ROC
c <- plot(rf_roc_nf, col = "red", lwd = 2, main = "Random Forest ROC Curve", print.auc = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")
```
Hyperparameter Tuning the RF Models

The tuneRF() function finds the optimal mtry (number of variables randomly sampled as candidates at each split) value for the model (with respect to Out-Of-Bag error estimates) for the specififed number of trees. Erika Torkildsen. Code below is self tested and tested on other machines successfully.

tuneRF for RF Model Filtered on Asthmatics with History of Vaping
```{r}
set.seed(13)
tuned_model <- tuneRF(
    x = train_bin[, predictors],
    y = train_bin$ECIG_BINARY,
    ntreeTry = 1000, # Number of trees
    mtryStart = 3, # starting value of mtry
    stepFactor = 1.5, # inflated or deflated by this at each iteration
    improve = 0.01, # improvement in OOB error
    trace = FALSE
)
```
tuneRF for RF Model Filtered on Asthmatics with History of Vaping and Conventional Smoking
```{r}
set.seed(14)
tuned_model_2 <- tuneRF(
    x = train_bin_2[, predictors],
    y = train_bin_2$ECIG_BINARY,
    ntreeTry = 1000, # Number of trees
    mtryStart = 3, # starting value of mtry
    stepFactor = 1.5, # inflated or deflated by this at each iteration
    improve = 0.01, # improvement in OOB error
    trace = FALSE
)
```
tuneRF for RF Model Filtered on Asthmatics
```{r}
set.seed(15)
tuned_model_nf <- tuneRF(
    x = train_bin_nf[, predictors],
    y = train_bin_nf$ECIG_BINARY,
    ntreeTry = 1000,
    mtryStart = 3,
    stepFactor = 1.5,
    improve = 0.01,
    trace = FALSE
)
```
Tuned RF Model Filtered on Asthmatics with History of Vaping and Smoking
```{r}
# Train Random Forest model
set.seed(16)
rf_model_n2 <- randomForest(
  x = train_bin_2[, predictors],
  y = train_bin_2$ECIG_BINARY,
  ntree = 1000,          # number of trees
  mtry = 3,              # number of variables per split (can tune)
  nodesize = 4,          # minimum size of terminal nodes
  importance = TRUE      # importance of predictors is assessed
)

# Check model summary
print(rf_model_n2)
```
New AUC for Model Filtered on Asthmatics with History of Vaping and Smoking
```{r}
# Predicted probabilities
rf_probs_n2 <- predict(rf_model_n2, type = "prob")[, 2]  # probability of class "1"

# ROC curve
rf_roc_n2 <- roc(as.numeric(as.character(train_bin_2$ECIG_BINARY)), rf_probs_n2, col = "green")

# AUC value
auc(rf_roc_n2)

# Plot ROC
b <- plot(rf_roc_2, col = "lightblue", lwd = 2, main = "Random Forest ROC Curve")
abline(a = 0, b = 1, lty = 2, col = "red")

b1 <- plot(rf_roc_n2, col = "blue", lwd = 2, main = "Random Forest ROC Curve", print.auc = TRUE, add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")

legend(x = "bottomright", legend = c("Original RF (History of Vaping and Smoking)", "Tuned RF (History of Vaping and Smoking)"), fill = c("lightblue", "blue"))
```
Tuned RF Model Filtered on Asthmatics
```{r}
# Train Random Forest model
set.seed(17)
rf_model_nf_2 <- randomForest(
  x = train_bin_nf[, predictors],
  y = train_bin_nf$ECIG_BINARY,
  ntree = 1000,          # number of trees
  mtry = 4,              # number of variables per split (can tune)
  nodesize = 4,          # minimum size of terminal nodes
  importance = TRUE      # importance of predictors is assessed
)

# Check model summary
print(rf_model_nf_2)
```

New AUC for Model Filtered on Asthmatics
```{r}
# Predicted probabilities
rf_probs_nf_2 <- predict(rf_model_nf_2, type = "prob")[, 2]  # probability of class "1"

# ROC curve
rf_roc_nf_2 <- roc(as.numeric(as.character(train_bin_nf$ECIG_BINARY)), rf_probs_nf_2)

# AUC
auc(rf_roc_nf)

# Plot ROC
c <- plot(rf_roc_nf, col = "pink", lwd = 2, main = "Random Forest ROC Curve")
abline(a = 0, b = 1, lty = 2, col = "red")

c_2 <- plot(rf_roc_nf_2, col = "red", lwd = 2, main = "Random Forest ROC Curve", print.auc = TRUE, add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")

legend(x = "bottomright", legend = c("Original RF (Asthmatics)", "Tuned RF (Asthmatics)"), fill = c("pink", "red"))
```
Addressing Class Imbalance and Overfitting

Class imbalance and overfitting is addressed with hyperparameters tuned using repeated cross-validation. Data is split into folds through the input specified by 'number' in the function. SMOTE generates synthetic samples of data and attempts to handle class imbalance. Erika Torkildsen. Code below is self tested and tested on other machines successfully.

Addressing Class Imbalance for Asthmatics with History of Vaping
```{r}
# Rename the factor levels
y <- factor(train_bin$ECIG_BINARY, levels = c("0", "1"), labels = c("No", "Yes"))

# Combine x_train and y_train into one data frame
data_rf <- data.frame(x, y)

# control function computation
set.seed(18)
ctrl_bin <- trainControl(method = "cv", #cross validation
                         number = 5, # 5 folds
                         sampling = "smote", # synthetic minority oversampling technique
                         classProbs = TRUE, # class probabilities computed in each resample
                         summaryFunction = twoClassSummary) # binary class
# apply to training data
randForMod <- train(y ~ .,
                       data = data_rf,
                       method = "rf",
                       trControl = ctrl_bin)

randForMod
```
Addressing Class Imbalance for Asthmatics with History of Vaping and Conventional Smoking
```{r}
# Rename the factor levels
y_2 <- factor(train_bin_2$ECIG_BINARY, levels = c("0", "1"), labels = c("No", "Yes"))

# Combine x_train and y_train into one data frame
data_rf_2 <- data.frame(x_2, y_2)

# control function computation
set.seed(19)
ctrl_bin_2 <- trainControl(method = "cv", #cross validation
                         number = 5, # 5 folds
                         sampling = "smote", # synthetic minority oversampling technique
                         classProbs = TRUE, # class probabilities computed in each resample
                         summaryFunction = twoClassSummary) # binary class
# apply to training data
randForMod_2 <- train(y_2 ~ .,
                    data = data_rf_2,
                    method = "rf",
                    trControl = ctrl_bin_2)

randForMod_2
```
Addressing Class Imbalance for Asthmatics
```{r}
# Rename the factor levels
y_nf <- factor(train_bin_nf$ECIG_BINARY, levels = c("0", "1"), labels = c("No", "Yes"))

# Combine x_train and y_train into one data frame
data_rf_nf <- data.frame(x_nf, y_nf)

# control function computation
set.seed(20)
ctrl_bin_nf <- trainControl(method = "cv", #cross validation
                         number = 5, # 5 folds
                         sampling = "smote", # synthetic minority oversampling technique
                         classProbs = TRUE, # class probabilities computed in each resample
                         summaryFunction = twoClassSummary) # binary class
# apply to training data
randForMod_nf <- train(y_nf ~ .,
                    data = data_rf_nf,
                    method = "rf",
                    trControl = ctrl_bin_nf)

randForMod_nf
```
Feature Importance

Understanding feature importance is crucial for enhancing model performance. Quality and quantity of feature importance has an impact on model accuracy. A variable importance plot is made for the three test cases, visualizing mean decrease accuracy and mean decrease in Gini. pch represents plotting character. Erika Torkildsen. Code below is self tested and tested on other machines successfully.

Feature Importance for Asthmatics with History of Vaping
```{r}
plot(randForMod)
varImpPlot(rf_model, main = "Variable Importance (Random Forest)", pch = 19, col = "black")
```
Feature Importance for Asthmatics with History of Vaping and Smoking
```{r}
plot(randForMod_2)
varImpPlot(rf_model_2, main = "Variable Importance (Random Forest)", pch = 19, col = "black")
```
Feature Importance for Asthmatics
```{r}
plot(randForMod_nf)
varImpPlot(rf_model_nf, main = "Variable Importance (Random Forest)", pch = 19, col = "black")
```
Predicting Test Data

Following computations made on the training data, the next step in the study involves making predictions on the test data using features. The predict() function takes in the random forest model that has undergone 5 fold cross-validation and the SMOTE technique as well as the test data as inputs. Probabilities are generated. Rick Morales. Code below is self tested and tested on other machines successfully.

Predictions for Asthmatics with History of Vaping
```{r}
predictor_vars <- c('AGE80', 'BMI', 'SMOKER_COMP', 'STATE', 'ADDEPEV', 'ALCDAY5',
                    'CHCCOPD', 'CHECKUP', 'EDUCA', 'GENHLTH', 'HLTHPLN', 'INCOME', 'SEX')
set.seed(21)
test_rand_pred <- predict(randForMod, test_bin)
test_bin_prob <- predict(randForMod, test_bin, type = "prob")
```
Predictions for Asthmatics with History of Vaping and Smoking
```{r}
set.seed(22)
test_rand_pred_2 <- predict(randForMod_2, test_bin_2)
test_bin_prob_2 <- predict(randForMod_2, test_bin_2, type = "prob")
```
Predictions for Asthmatics
```{r}
set.seed(23)
test_rand_pred_nf <- predict(randForMod_nf, test_bin_nf)
test_bin_prob_nf <- predict(randForMod_nf, test_bin_nf, type = "prob")
```

Precision Recall

Precision recall visualizes the tradeoff between precision (accuracy of positive predictors) and recall (ability to find positive cases) and is used best on imbalanced datasets. PR-AUC focuses on the positive class. Scores and weights are called by pr.curve. Rick Morales and Stephanie Halsing. Code below is self tested and tested on other machines successfully.

Precision Recall for Asthmatics with History of Vaping
```{r}
# Filter test_bin to only rows with complete predictor values
test_bin_clean <- test_bin %>%
  filter(if_all(all_of(predictor_vars), ~ !is.na(.) & is.finite(.)))

# Predict probabilities
predict_prob <- predict(randForMod, newdata = test_bin_clean, type = "prob")

# Convert ECIG_BINARY to numeric labels (0 or 1)
weights <- as.numeric(as.character(test_bin_clean$ECIG_BINARY) == "1")

# Combine and drop NAs
pr_data <- data.frame(prob = predict_prob[, 2], label = weights)
pr_data <- pr_data[complete.cases(pr_data), ]

# Run PR Curve
pr <- pr.curve(scores.class0 = pr_data$prob,
               weights.class0 = pr_data$label,
               curve = TRUE)

# Plot it
plot(pr)

pr$auc.integral  # This is your PR-AUC score
```
Precision Recall for Asthmatics with History of Vaping and Conventional Smoking
```{r}
# Filter test_bin to only rows with complete predictor values
test_bin_clean_2 <- test_bin_2 %>%
  filter(if_all(all_of(predictor_vars), ~ !is.na(.) & is.finite(.)))

# Predict probabilities
predict_prob_2 <- predict(randForMod_2, newdata = test_bin_clean_2, type = "prob")

# Convert ECIG_BINARY to numeric labels (0 or 1)
weights_2 <- as.numeric(as.character(test_bin_clean_2$ECIG_BINARY) == "1")

# Combine and drop NAs
pr_data_2 <- data.frame(prob = predict_prob_2[, 2], label = weights_2)
pr_data_2 <- pr_data_2[complete.cases(pr_data_2), ]

# Run PR Curve
pr_2 <- pr.curve(scores.class0 = pr_data_2$prob,
               weights.class0 = pr_data_2$label,
               curve = TRUE)

# Plot it
plot(pr_2)

pr_2$auc.integral  # This is your PR-AUC score
```
Precision Recall for Asthmatics
```{r}
# Filter test_bin to only rows with complete predictor values
test_bin_clean_nf <- test_bin_nf %>%
  filter(if_all(all_of(predictor_vars), ~ !is.na(.) & is.finite(.)))

# Predict probabilities
predict_prob_nf <- predict(randForMod_nf, newdata = test_bin_clean_nf, type = "prob")

# Convert ECIG_BINARY to numeric labels (0 or 1)
weights_nf <- as.numeric(as.character(test_bin_clean_nf$ECIG_BINARY) == "1")

# Combine and drop NAs
pr_data_nf <- data.frame(prob = predict_prob_nf[, 2], label = weights_nf)
pr_data_nf <- pr_data_nf[complete.cases(pr_data_nf), ]

# Run PR Curve
pr_nf <- pr.curve(scores.class0 = pr_data_nf$prob,
               weights.class0 = pr_data_nf$label,
               curve = TRUE)

# Plot it
plot(pr_nf)

pr_nf$auc.integral  # This is your PR-AUC score
```
Multi-Class Logistic Regression Model

A second target variable is explored to compare effects on performance against the binary. Methods used for binary target are employed for multi-class. The multi-class target is filtered on adults with history of asthma. Erika Torkildsen. Code below is self tested and tested on other machines successfully.

# Class Imbalance Check Multi Class
```{r}
# Binary Variable Creation
df_lr <- brfss_2017_cleaned

# Filter on adult asthmatics
df_lr <- brfss_2017_cleaned %>%
    filter(ASTHMA == 1)

df_lr$ECIG_COMP <- as.factor(df_lr$ECIG_COMP)

#train-test split with 1 partition, 70% of data in training, and results not stored in list
set.seed(21)
split_lr <- createDataPartition(df_lr$ECIG_COMP, times = 1, p = 0.7, list = FALSE)
train_lr <- df_lr[split_lr, ]
test_lr <- df_lr[-split_lr, ]
```

Imputation of Missing Values for Multi-Class Target
```{r}
# Calculate imputed values with training data
methods(mice)

# Calculate imputed values with mice() using cart with 1 multiple imputation and 1 iteration. Reproducible.
imp_multi <- mice(train_lr, method = "cart", m = 1, maxit = 1, seed = 22)

# Complete training data with imputed values
train_lr <- complete(imp_multi)
```
Class Imbalance Check for Multi-Class Filtered on Asthmatics
```{r}
p4 <- ggplot(train_lr, aes(x = as.factor(ECIG_COMP), 
                      y = prop.table(stat(count)),
                      fill = as.factor(ECIG_COMP),
                      label = scales::percent(prop.table(stat(count))))) + 
    geom_bar(position = "dodge") +
    geom_text(stat = 'count',
              position = position_dodge(.10),
              vjust = -0.5,
              size = 4) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_discrete(labels = c('Every Day', 'Some Days', 'Former', 'Never Used')) + 
    labs(title = "E-Cigarette Smoking Status Among Asthmatics 2017", x = "E-Cig Smoking Status", y = "Surveyed Population %", fill = "E-Cig Smoking Status")

p4
```
Ordinal Logistic Regression

An ordinal logistic regression model is fit for the multi-class target. It is used for ordinal categorical outcomes and ideal for multiple classes. A predictror matrix is created prior to building the model. Erika Torkildsen and Stephanie Halsing. Code below is self tested and tested on other machines successfully.

# Model Fitting (Ordinal Logistic Regression)
```{r}
x_train <- data.matrix(train_lr[, predictor_vars])
y_train <- train_lr$ECIG_COMP

# Create predictor matrix
x <- data.matrix(train_lr[, predictor_vars])

# Target
y <- train_lr$ECIG_COMP

# Final modeling data frame
data <- data.frame(x, y)

# Revalue factor levels
y_train <- revalue(y_train, c("1" = "X1", "2" = "X2", "3" = "X3", "4" = "X4"))
levels(y_train)

# Combine x_train and y_train into one data frame
train_lm <- data.frame(y_train, x_train)

set.seed(23)
model_lr <- polr(y_train ~ ., data = train_lm)
model_lr
```
View model results and variance inflation factors
```{r}
summary(model_lr)
model_parameters(model_lr)
vif(model_lr)
```
Addressing Class Imbalance for Asthmatics (Multi-class)
```{r}
# Ensure complete cases
train_lm <- train_lm[complete.cases(train_lm), ]

# control function optimization
set.seed(24)
ctrl <- trainControl(method = "cv", #cross validation
                     number = 5, # 5 folds
                     sampling = "smote", # synthetic minority oversampling technique
                     classProbs = TRUE, # class probabilities computed in each resample
                     summaryFunction = multiClassSummary) # multi-class
# apply to training data
ordLogMod <- train(y_train ~ .,
                       data = train_lm,
                       method = "polr",
                       trControl = ctrl)
ordLogMod
```
Predictions for Asthmatics and ROC curve
```{r}
# Filter test_bin to only rows with complete predictor values
test_lr <- test_lr %>%
  filter(if_all(all_of(predictor_vars), ~ !is.na(.) & is.finite(.)))

# Predict probabilities
predict_lr <- predict(ordLogMod, newdata = test_lr)
predict_prob_lr <- predict(ordLogMod, newdata = test_lr, type = "prob")

# Convert ECIG_BINARY to numeric labels (0 or 1)
weights_lr <- as.numeric(as.character(test_lr$ECIG_COMP) == "1")

# Combine and drop NAs
pr_data_lr <- data.frame(prob = predict_prob_lr[, 2], label = weights_lr)
pr_data_lr <- pr_data_lr[complete.cases(pr_data_lr), ]

# Run PR Curve
pr_lr <- pr.curve(scores.class0 = pr_data_lr$prob,
               weights.class0 = pr_data_lr$label,
               curve = TRUE)

# Plot it
plot(pr_lr)

pr$auc.integral  # This is your PR-AUC score
```
ROC curve computation for multi-class. Erika Torkildsen. Code below is self tested and tested on other machines successfully.
```{r}
# ROC curve
rf_probs_lr <- predict(model_lr, type = "prob")[, 2]  # probability of class "1"
rf_roc_lr <- roc(as.numeric(as.character(train_lr$ECIG_COMP)), rf_probs_lr)

# AUC
auc(rf_roc_lr)

# Plot ROC
d <- plot(rf_roc_lr, col = "forestgreen", lwd = 2, main = "Ordinal Logistic Regression ROC Curve", print.auc = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")
```
Combine all ROC curves. All curves from 4 models in total overlaid on one plot. Erika Torkildsen. Code below is self tested and tested on other machines successfully.
```{r}
# Asthmatics and vaping (binary)

# Plot ROC
a <- plot(rf_roc, col = "blue", lwd = 2, main = "Random Forest ROC Curve")
abline(a = 0, b = 1, lty = 2, col = "red")

# Asthmatics and vaping and smoking (binary)

# Plot ROC
b <- plot(rf_roc_2, col = "forestgreen", lwd = 2, main = "Random Forest ROC Curve", add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")

# Asthmatics (binary)

# Plot ROC
c <- plot(rf_roc_nf, col = "red", lwd = 2, main = "Random Forest ROC Curve", print.auc = TRUE, add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")

# multi class
# ROC curve
rf_probs_lr <- predict(model_lr, type = "prob")[, 2]  # probability of class "1"
rf_roc_lr <- roc(as.numeric(as.character(train_lr$ECIG_COMP)), rf_probs_lr)

# AUC
auc(rf_roc_lr)

# Plot ROC
d <- plot(rf_roc_lr, col = "orange", lwd = 2, main = "Ordinal Logistic Regression ROC Curve", add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "red")

legend(x = "bottomright", legend = c("Asthmatics and Vaping (Binary)", "Asthmatics, Vaping and Smoking (Binary)", "Asthmatics (Binary)", "Asthmatics (Multi-Class"), fill = c("blue", "forestgreen", "red", "orange"))
```
